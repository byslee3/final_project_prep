

=============================

Tuesday - Nov 6

Reconstructing database schema by getting a JSON string from URL, then matching it against what shows up on the webpage. In order to understand what data there is in Polyvore.

Sketching out ideas for UX and what to do with this data. Idea of outfit recommendations using k-nearest neighbors algorithm (would have to pull a dataset that forces there to be lots of overlap between users). Or, using the fact that Polyvore sets are human-curated groupings of items --> create a "social graph" of outfit items.

What to do??

Inspirational Polyvore set: http://www.polyvore.com/sin_t%C3%ADtulo_841/set?id=41830128#stream_box

=============================

Wednesday - Nov 7

Talked about tree structure as way to store Polyvore data of connections between items.

The core value of the Polyvore data set is human-curated groupings.

Started going through the JSON strings and matching it to what's on the page. Coding up a dictionary to store all of these.

UX idea and this is how you would incorporate some predictive functionality: Asking them to check off which items are in their closet. You can't realistically surface 100+ items to them all at once and ask them to choose which ones are in their closet. You have to start with 20 items, see which ones they check off as they have them, then continue to surface groups of 20, tailoring it based on what they said they had before.

UX feature: After you know what they have in their closet -- use that data to suggest 3 items that would complete their closet -- multiply their amount of available outfit combos by 10x, just by adding 3 items.

User has to experience feeling of "magic" -- and "trust." I want them to feel like they are talking to an very knowledgeable wardrobe stylist. Same feeling as going to the doctor. Like "tell me what your problem is, and I will take care of you." I want them to "trust" the computer and be delighted by the recommendations ("How did it read my mind?! I never would have thought of that, but it's exactly what I want!").

=============================

Thursday - Nov 8

Continue to code stuff to get from {webpage} ---> {json string} ---> {Python dictionary that I can manipulate}
Spending most time on understanding how they've structured their database.

Issues addressed/encountered:
* Set page only shows the first 18 fans. How to get all of them? --> Guessing the URL and pulling from that, wrote script to do this
* Should I populate database as I go, or save all JSON strings and parse through it later? --> Save all strings in case I need to go back later.
* Found out where Polyvore has this data (yay!): Denoting which set items are products versus graphics, some category tags and category ids for each item (need to figure out which is most predictive/relevant)

* Should I pull the relationship between users who saved, and items?
  Or just keep item <---> set relationship and focus on that?

Issues still to resolve:
* What's the right order for crawling through the pages?
    >>> resolved: see notebook sketches
* Store the data in text files or CSV? >>> Text files as CSV gets weird if more than 2GB. Come up with a file directory system so that max 30K files in any folder.
* Set up database schema and write script to populate the database

=============================

Friday - Nov 9

* Set up database using SQLite3 and SQLalchemy. Decided not to use SQLalchemy for this part since most of it needs to be written directly in SQL anyway.
* Script: Pulled 5 top sets and save them in text files -- Pull sets from Tuesday so they have had maximum time for exposure -- In order to get all the possible fans
* Wrote script to pull data from text files and store it in the database
* Started building out seed.py

=============================

Monday - Nov 12

For each object (set, user, item) the following code must be written (see sketch in notebook):
-- Start with the id + seo title in some cases
-- Guess the appropriate URL, looking at examples from Polyvore
-- Grab the JSON file and write it to a text file
-- Parse through JSON and grab the data that we need
-- Write it to the SQL database
-- Loop back to the beginning: from the SQL database pull everything that this object is linked to, and then start over at the beginning of these instructions

Functions written today:
* Given a set, pull all of the fan_ids associated with it.
* Given a set, pull all of the item_ids associated with it.
* Set up database and populated the test data into it.
* Ran prelim analysis to test degree of overlap in original 5 sets.
* Continue building out seed.py

What is slowing me down -- thinking about how to structure the code. Concern over whether I am following best practices. Continually going back and revising code.

=============================

Tuesday - Nov 13

Functions written today:
* Given an item, pull all of the sets that it belongs to. Store in database.
* Given a user, pull 50 of the sets they Liked. Pull 100 of the items they Liked.
* Populate the database.
* Come up with file directory scheme so that no folder has more than 30K files in it and revise previous code.
* Clean up directories and files in preparation for pulling full dataset.

In order to move faster -- unless it's very obvious, I am not worrying about code abstractions for now. The code is more verbose right now, but I can always go back and edit later.

=============================

Thursday - Nov 15

* Pulled data up through level 2 (writing functions along the way)
* Wrote scripts for populating database with basic item & fan attributes

=============================

Friday - Nov 16

* Ideas for data mining and analyses
* How to visualize the data mining
* Brainstorm on front end --> came up with simple/working concept that can then be refined
* Continued to pull data up to level 3

=============================

Monday - Nov 19

* Built skeleton for web app
* Finished pulling item data to level 3. ~37K records. Need to clean it tomorrow.
* Learning Javascript/Jquery...

=============================

Tuesday - Nov 20

Continued to do data manipulations:
* Reverse populated Items_Sets and added a field for imgurl so we don't have to pull Set files
* Populated the Sets file

List of data cleaning analyses / debugging:
-- Got errors with some files when populating items into database. Found a consistent 3% error rate which is acceptable for my purposes right now. Found out this problem was due to non-English filenames, so the files never loaded from Polyvore (because I used Try/Except UnicodeError).
-- Some records in Items have a price (so they were originally marked as product and pulled) however they don't have a USD price so they look like bad records in the database. This is about 15% of all Item records. Spot checked this group (usd_price = -1) against the other group (usd_price > 0) and determined this is a good proxy for whether it's a good record or not. So, cleaning out all records with usd_price < 0.

Check and clean each database:
* Sets (405,030 records | 404,099 unique sets)
* Items (41,812 records | 41,797 unique items)
* Users (1,171 records | 1,171 unique users)
-------------------------------------------------------------------------
* Sets_Fans (2,154 records | 1,171 unique users | 5 unique sets)
* Fans_Sets (we ended up not populating this)
-------------------------------------------------------------------------
* Items_Sets (538,845 records | 36,615 unique items | 404,099 unique sets)
* Sets_Items (32 records | 32 unique items | 5 unique sets)
-------------------------------------------------------------------------
* Users_Items (55,476 records | 1,120 unique users | 43,045 unique items)
* Items_Users (we ended up not populating this)

Could go back and pull these relationships (but we won't do it now due to time constraints):
* Level 3 sets --> pull associated fans
* Level 3 sets --> pull associated items
* All of the Items --> Users associations
* We stopped at Level 4 sets but could continue
Basically anything dotted in green in the notebook.

Data in the overlapping tables is too little, doesn't make sense to merge it right now.
Here are the tables and relationships that we have significant data for right now:
--------------------------------------------------------------------------------------------
* SETS (405,030 records | 404,099 unique sets)
* ITEMS (41,812 records | 41,797 unique items)
* USERS (1,171 records | 1,171 unique users)
--------------------------------------------------------------------------------------------
* ITEMS <--> SETS   Items_Sets (538,845 records | 36,615 unique items | 404,099 unique sets)
* ITEMS <--> USERS  Users_Items (55,476 records | 1,120 unique users | 43,045 unique items)

Additional data cleaning from this point:
-- Clean out any bad data from each of the tables
        Remove any records from Items that have usd_price < 0 
-- Remove duplicates
-- Run inner joins and left/right joins to check that data can be linked correctly
-- Save this as the final database

=============================

Wednesday - Nov 21

* Started building out the engine for the web interface.
* Decided to leave database as is, since more errors are going to come up in the course of writing the rest of the program. Use this as test data now to get the program running, then clean the database.
* Turned out to be a good idea... because it turns out this data is kind of useless for our purposes. Sets and items do not overlap enough for our original matching algorithm to work.
* Need more thinking on how we can salvage this.

=============================

Thanksgiving - Thursday to Sunday - Nov 22 to 25

Thought about how this problem can be solved.
-- Reframed the original problem: Assume there are no novel combinations, the only valid combinations are those that are already laid out in Polyvore sets.
-- For the next step: Assuming there are novel combinations -- how can we find them? Brute force: come up with every possible combination depending on what the user selected. Then, match those against a set of criteria that we've defined as = a valid set. If they don't meet those criteria, then throw them out.
    ----> How will we come up with this criteria? Based on characteristics of Polyvore sets. But, we need some human guidance as to which criteria to test for.
More notes in the notebook.

"Keep things simple, then expand." --Dad

=============================

Monday - Nov 26

* Pull just the sets with 4+ items, to give us a working data set for testing our new algorithm
* Write out the matching algorithm just in Python, no web interface. Test this in Python first
    ---> Method for selecting the starting inventory
    ---> Method for matching sets

Learning: Always just do the minimum necessary, instead of investing a lot of time trying to make it perfect at each step. Always go back and improve it later, but build out a test case first, so you don't waste your time trying to perfect something that ultimately doesn't work. Always do a test case first!!!!!

=============================

Tuesday - Nov 27

* Finish writing Python interface for matching sets

=============================

Wednesday - Nov 28 - Only had about 2 hours

* Write the part where they can choose additional items and see how that changes the available sets

=============================

Thursday - Nov 29

* Installed environment on my laptop

=============================

Friday - Nov 30

* Debugging the "select additional items" part
    Problem was with list references: Iterating through a list while deleting parts of it
    And also how sometimes variables are just POINTERS to a location, not actually creating new data
    Seems to be true in the case of a dictionary? So you need to be careful about changing the original location

* Added feature to clear the hypothetical selections and start over from your declared inventory

=============================

Saturday - Dec 1

start doing some basic things in bootstrap to get comfortable with it





=============================
Finish by Monday:
---> Refine the process for "Suggested Items"
---> Figure out how to make the pictures show up
---> Make it look good using Bootstrap
---> If time, figure out how to host it

=============================
Finish in future:
---> Pull more data *** When re-pulling this data start from 100 items that I think are staples
     That way we can be sure of having a starter group of items that works
---> Start on data mining










=============================================
=============== Summarized ==================
=============================================

Reconstruct the Polyvore database
--- Understand structure of JSON strings and match to what's shown on the webpage
--- Write script to traverse through linked pages, pull out JSON strings, and store them in text files. Need to traverse through linked pages (not just randomly) so we can force the data to have connections.
--- Write script to parse JSON text files, pull out the data we need, and stick it in a database
--- Create database schema and populate the database
--- Clean data and debug processes



=============================================
=============== Challenges ==================
=============================================

* More and more problems unfolding -- grid structure
    For example, needed to pull in groups of 8 to make it work within the grid structure
    Needed to format them into groups
        Then what happens if you have something that's not a multiple of 8?
            Then you have to pull extra items to make up the difference
                But then you also have to check against the existing items you have
                    And then you have to format the images so they fit correctly
* Getting random items --> this is a place I could optimize later
    Should I query the database and get all the distinct ids? (long runtime)
    Or should I get a random number first and just pull those that match? But we don't know the population set of the item_ids
* Coming up with a file directory system when pulling from Polyvore
    Since each folder on the desktop can only store 30K files
    Using the last digit of each id to randomly distribute files into folders
* Picking the suggested items --> will need to think about how to optimize for runtime
* The algorithm for matching sets --> also think about runtime implications
* Each step unfolds to more steps
    Like the first example above.
    And also how long it took to pull the data originally.
    And then it turned out the user behavior was not like I wanted.
    And I had to re-pull the data.




















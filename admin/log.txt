

=============================

Tuesday - Nov 6

Reconstructing database schema by getting a JSON string from URL, then matching it against what shows up on the webpage. In order to understand what data there is in Polyvore.

Sketching out ideas for UX and what to do with this data. Idea of outfit recommendations using k-nearest neighbors algorithm (would have to pull a dataset that forces there to be lots of overlap between users). Or, using the fact that Polyvore sets are human-curated groupings of items --> create a "social graph" of outfit items.

What to do??

Inspirational Polyvore set: http://www.polyvore.com/sin_t%C3%ADtulo_841/set?id=41830128#stream_box

=============================

Wednesday - Nov 7

Talked about tree structure as way to store Polyvore data of connections between items.

The core value of the Polyvore data set is human-curated groupings.

Started going through the JSON strings and matching it to what's on the page. Coding up a dictionary to store all of these.

UX idea and this is how you would incorporate some predictive functionality: Asking them to check off which items are in their closet. You can't realistically surface 100+ items to them all at once and ask them to choose which ones are in their closet. You have to start with 20 items, see which ones they check off as they have them, then continue to surface groups of 20, tailoring it based on what they said they had before.

UX feature: After you know what they have in their closet -- use that data to suggest 3 items that would complete their closet -- multiply their amount of available outfit combos by 10x, just by adding 3 items.

User has to experience feeling of "magic" -- and "trust." I want them to feel like they are talking to an very knowledgeable wardrobe stylist. Same feeling as going to the doctor. Like "tell me what your problem is, and I will take care of you." I want them to "trust" the computer and be delighted by the recommendations ("How did it read my mind?! I never would have thought of that, but it's exactly what I want!").

=============================

Thursday - Nov 8

Continue to code stuff to get from {webpage} ---> {json string} ---> {Python dictionary that I can manipulate}
Spending most time on understanding how they've structured their database.

Issues addressed/encountered:
* Set page only shows the first 18 fans. How to get all of them? --> Guessing the URL and pulling from that, wrote script to do this
* Should I populate database as I go, or save all JSON strings and parse through it later? --> Save all strings in case I need to go back later.
* Found out where Polyvore has this data (yay!): Denoting which set items are products versus graphics, some category tags and category ids for each item (need to figure out which is most predictive/relevant)


* Should I pull the relationship between users who saved, and items?
  Or just keep item <---> set relationship and focus on that?


Issues still to resolve:
* What's the right order for crawling through the pages?
    >>> resolved: see notebook sketches
* Store the data in text files or CSV? >>> Text files as CSV gets weird if more than 2GB. Come up with a file directory system so that max 30K files in any folder.
* Set up database schema and write script to populate the database

=============================

Friday - Nov 9

* Set up database using SQLite3 and SQLalchemy. Decided not to use SQLalchemy for this part since most of it needs to be written directly in SQL anyway.
* Script: Pulled 5 top sets and save them in text files -- Pull sets from Tuesday so they have had maximum time for exposure -- In order to get all the possible fans
* Wrote script to pull data from text files and store it in the database
* Started building out seed.py

=============================

Monday - Nov 12

For each object (set, user, item) the following code must be written (see sketch in notebook):
-- Start with the id + seo title in some cases
-- Guess the appropriate URL, looking at examples from Polyvore
-- Grab the JSON file and write it to a text file
-- Parse through JSON and grab the data that we need
-- Write it to the SQL database
-- Loop back to the beginning: from the SQL database pull everything that this object is linked to, and then start over at the beginning of these instructions

Functions written today:
* Given a set, pull all of the fan_ids associated with it.
* Given a set, pull all of the item_ids associated with it.
* Set up database and populated the test data into it.
* Ran prelim analysis to test degree of overlap in original 5 sets.
* Continue building out seed.py

What is slowing me down -- thinking about how to structure the code. Concern over whether I am following best practices. Continually going back and revising code.

=============================

Tuesday - Nov 13

Functions written today:
* Given an item, pull all of the sets that it belongs to. Store in database.
* Given a user, pull 50 of the sets they Liked. Pull 100 of the items they Liked.
* Populate the database.
* Come up with file directory scheme so that no folder has more than 30K files in it and revise previous code.
* Clean up directories and files in preparation for pulling full dataset.

In order to move faster -- unless it's very obvious, I am not worrying about code abstractions for now. The code is more verbose right now, but I can always go back and edit later.

=============================

Thursday - Nov 15

* Pulled data up through level 2 (writing functions along the way)
* Wrote scripts for populating database with basic item & fan attributes

=============================

Friday - Nov 16

* Ideas for data mining and analyses
* How to visualize the data mining
* Brainstorm on front end --> came up with simple/working concept that can then be refined
* Continued to pull data up to level 3

=============================

Monday - Nov 19

* Built skeleton for web app
* Finished pulling item data to level 3. ~37K records. Need to clean it tomorrow.
* Learning Javascript/Jquery...

=============================

Tuesday - Nov 20

Continued to do data manipulations:
* Reverse populated Items_Sets and added a field for imgurl so we don't have to pull Set files
* Populated the Sets file


List of data cleaning analyses / debugging:
-- Got errors with some files when populating items into database. Found a consistent 3% error rate which is acceptable for my purposes right now. Found out this problem was due to non-English filenames, so the files never loaded from Polyvore (because I used Try/Except UnicodeError).
-- Some records in Items have a price (so they were originally marked as product and pulled) however they don't have a USD price so they look like bad records in the database. This is about 15% of all Item records. Spot checked this group (usd_price = -1) against the other group (usd_price > 0) and determined this is a good proxy for whether it's a good record or not. So, cleaning out all records with usd_price < 0.


Check and clean each database:
* Sets (405,030 records | 404,099 unique sets)
* Items (41,812 records | 41,797 unique items)
* Users (1,171 records | 1,171 unique users)
-------------------------------------------------------------------------
* Sets_Fans (2,154 records | 1,171 unique users | 5 unique sets)
* Fans_Sets (we ended up not populating this)
-------------------------------------------------------------------------
* Items_Sets (538,845 records | 36,615 unique items | 404,099 unique sets)
* Sets_Items (32 records | 32 unique items | 5 unique sets)
-------------------------------------------------------------------------
* Users_Items (55,476 records | 1,120 unique users | 43,045 unique items)
* Items_Users (we ended up not populating this)


Could go back and pull these relationships (but we won't do it now due to time constraints):
* Level 3 sets --> pull associated fans
* Level 3 sets --> pull associated items
* All of the Items --> Users associations
* We stopped at Level 4 sets but could continue
Basically anything dotted in green in the notebook.


Data in the overlapping tables is too little, doesn't make sense to merge it right now.
Here are the tables and relationships that we have significant data for right now:
--------------------------------------------------------------------------------------------
* SETS (405,030 records | 404,099 unique sets)
* ITEMS (41,812 records | 41,797 unique items)
* USERS (1,171 records | 1,171 unique users)
--------------------------------------------------------------------------------------------
* ITEMS <--> SETS   Items_Sets (538,845 records | 36,615 unique items | 404,099 unique sets)
* ITEMS <--> USERS  Users_Items (55,476 records | 1,120 unique users | 43,045 unique items)


Additional data cleaning from this point:

-- Clean out any bad data from each of the tables
        Remove any records from Items that have usd_price < 0 
-- Remove duplicates
-- Run inner joins and left/right joins to check that data can be linked correctly
-- Save this as the final database















=============================================
=============== Summarized ==================
=============================================

Reconstruct the Polyvore database
--- Understand structure of JSON strings and match to what's shown on the webpage
--- Write script to traverse through linked pages, pull out JSON strings, and store them in text files. Need to traverse through linked pages (not just randomly) so we can force the data to have connections.
--- Write script to parse JSON text files, pull out the data we need, and stick it in a database
--- Create database schema and populate the database
--- Clean data and debug processes